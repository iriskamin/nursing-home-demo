# Evaluation Strategy & Results
## Nursing Home RAG Demo

---

## Overview

This document describes the evaluation framework used to measure the quality and correctness of the Nursing Home RAG Demo. The evaluation focuses on two key dimensions:

1. **Classifier Accuracy** — Does the system route queries to the correct retrieval method?
2. **Retrieval Quality** — Are the returned results relevant and accurate?

---

## Evaluation Components

### 1. Golden Test Set

A curated set of 15 test queries stored in `outputs/golden_test_set.csv`. Each query has:

| Field | Description |
|---|---|
| `query` | The user question to send to the system |
| `expected_retrieval_type` | The correct routing: SQL, VECTOR, or HYBRID |
| `expected_result_contains` | Key data elements that should appear in results |
| `pass_criteria` | Human-readable description of what a correct answer looks like |

The 15 queries are evenly distributed:
- **5 SQL queries** — structured filtering (ratings, fines, state, counts)
- **5 VECTOR queries** — semantic/keyword search over deficiency narratives
- **5 HYBRID queries** — combining structured filters with narrative search

### 2. Automated Evaluation Script

`outputs/evaluate_demo.py` automates the evaluation process:

- Reads the golden test set
- Sends each query to the live Dify API
- Parses the response to detect which retrieval method was used
- Compares detected method to expected method
- Computes accuracy metrics
- Generates detailed reports

**Usage:**
```
python evaluate_demo.py              # Run against live Dify API
python evaluate_demo.py --mock       # Run with synthetic responses (no API calls)
python evaluate_demo.py --delay 3    # Custom delay between API calls
```

**Outputs:***
- `outputs/eval_runs/eval_YYYYMMDD_HHMMSS.csv` — per-query detailed results
- `outputs/eval_runs/eval_YYYYMMDD_HHMMSS.txt` — formatted summary report with metrics and confusion matrix

### 3. Langfuse Observability

Every query processed by the system generates a trace in Langfuse, providing:
- Full node-by-node execution breakdown
- Token usage and cost per node
- Latency for each step
- Which retrieval branch was triggered
- The final synthesized answer

#### Dify Logs vs. Langfuse Traces
- **Dify Logs (`Logs & Ann.` in Dify):** High-level view showing the user input and final assistant output. Useful for understanding *what* users are asking. (Note: Queries sent via the evaluation script appear here, tagged with `user: eval-script`).
- **Langfuse Traces:** Deep-dive observability tool showing the exact step-by-step waterfall execution (prompt routing, SQL generated, vector chunks returned). Used to understand *why* the system produced a specific answer or *how* it failed.

#### Trace Types in Langfuse
- `Message`: The end-to-end execution of the primary chatflow to answer the user's question. **This is the trace you evaluate.**
- `generate_conversation_name`: A background system trace generated by Dify to title the chat session in the UI. **Ignore this during evaluation.**

---

## Evaluation Metrics

### Classifier Accuracy

Measures whether the Query Classifier LLM correctly routes queries to SQL, VECTOR, or HYBRID retrieval.

| Metric | Target | How Measured |
|---|---|---|
| Overall accuracy | > 85% | Correct classifications / total evaluated queries |
| Per-type accuracy | > 80% each | Accuracy broken down by SQL, VECTOR, HYBRID |

### Retrieval Quality Metrics

| Metric | Target | How Measured |
|---|---|---|
| Relevance@5 | > 3/5 relevant | For VECTOR queries, manually score top 5 returned chunks |
| SQL correctness | > 90% | Verify returned facilities match the filter criteria |
| Hybrid completeness | 100% | HYBRID responses show evidence of both retrieval methods |
| Faithfulness | > 0.8 | LLM-as-judge in Langfuse Evals (no hallucinated data) |

### Langfuse Score Types

Five score types configured in Langfuse for manual and automated scoring:

| Score Name | Scale | What It Measures |
|---|---|---|
| `faithfulness` | 0–1 | Answer only uses retrieved content (no hallucination) |
| `relevance` | 0–1 | Answer addresses the user's actual question |
| `retrieval_mode_correct` | 0 or 1 | System used the correct retrieval method |
| `sql_accuracy` | 0 or 1 | SQL filter returned correct results |
| `answer_completeness` | 0–1 | Captured all key findings from retrieved content |

#### Scoring Rubric (Binary vs. Decimal)
Some metrics should be scored as strict binary (0 or 1), while others allow nuance (decimals between 0 and 1).

*   **Objective/Binary Scores (Use strictly 0 or 1):**
    *   `retrieval_mode_correct`: The query was routed correctly (1) or it wasn't (0).
    *   `sql_accuracy`: The database executed the correct SQL and returned valid data (1) or it threw an error/returned wrong data (0).
    *   `faithfulness`: **Strict compliance required.** If the LLM hallucinates *even one sentence* that is not supported by the context, the entire answer receives a 0.
*   **Subjective Scores (Use decimals like 0.5, 0.8):**
    *   `relevance`: How well the answer directly addressed the question (e.g., 0.7 if slightly off-topic).
    *   `answer_completeness`: How much of the necessary data was returned (e.g., 0.5 if it answered half the question).

#### How to Define Score Types in Langfuse

1. Go to https://cloud.langfuse.com and open your project
2. Navigate to **Settings** → **Scores** (in the left sidebar)
3. Click **Add new score config**
4. For each score type above, fill in:
   - **Name** — the score name exactly as listed (e.g. `faithfulness`)
   - **Data Type** — select `Numeric`
   - **Min Value** — `0`
   - **Max Value** — `1`
   - **Description** — copy the "What It Measures" text from the table above
5. Click **Save** to create the score type
6. Repeat for all 5 score types

For more details on Langfuse scores, see: https://langfuse.com/faq/all/what-are-scores

---

## Evaluation Process — Step by Step

### Step 1: Run the Automated Evaluation

```
cd outputs
python evaluate_demo.py
```

This sends all 15 golden queries through the live Dify app and generates the timestamped results CSV and text report in `outputs/eval_runs/`.

### Step 2: Review the Evaluation Report

Open the latest generated text report (e.g., `outputs/eval_runs/eval_YYYYMMDD_HHMMSS.txt`) and review:
- **Overall accuracy** — should be above 85%
- **Per-type accuracy** — check if any category is underperforming
- **Confusion matrix** — identify systematic misclassifications
- **Error list** — investigate any API errors or incorrect predictions

### Step 3: Review Traces in Langfuse

1. Go to https://cloud.langfuse.com → your project → Traces
2. Pay attention to the **Cost & Latency coloring**:
   - **Green:** Typical performance (< 75th percentile).
   - **Yellow:** Slower/more expensive than average (75th - 90th percentile).
   - **Red:** Critical outlier (> 90th percentile). Investigate these for bloated context windows or slow DB queries.
3. For each test query, verify:
   - The classifier node output matches expected retrieval type
   - The correct retrieval branch fired
   - The final answer uses only retrieved data (no hallucination)
   - Token cost and latency are within acceptable ranges

#### Debugging Trace Errors
If a trace has failed (or the Python script reports an `HTTP_400`), find the root cause in Langfuse:
1. Filter traces by `Level = ERROR`.
2. Open the trace and view the Waterfall/Tree chart.
3. Click the specific node marked with a red `[!]` or `ERROR`.
4. Read the `Output` or `Logs` tab on the right side to see the raw exception (e.g., `{"error": "column 'rating' does not exist"}`).
5. *Note:* If Dify "swallows" the error before sending it to Langfuse, you may need to check the **Run History** directly inside Dify Studio to see the raw error traceback.

### Step 4: Manual Scoring in Langfuse

For each trace, apply manual scores:

1. Go to **Traces** in the left sidebar and click on a trace to open it
2. In the trace detail view, click the **Annotate** button (top right area)
3. You will see all the score types you defined in Settings
4. For each score, enter a value between 0 and 1:
   - **faithfulness** — 1.0 if the answer only uses data from retrieved content, lower if it hallucinated
   - **relevance** — 1.0 if the answer directly addresses the user's question, lower if off-topic
   - **retrieval_mode_correct** — 1 if the correct retrieval method (SQL/VECTOR/HYBRID) was used, 0 if not
   - **sql_accuracy** — 1 if the SQL filter returned correct results, 0 if wrong (only for SQL/HYBRID queries)
   - **answer_completeness** — 1.0 if all key findings were captured, lower if partial
5. Click **Save** to store the scores
6. After scoring all traces, review aggregate scores on the Langfuse **Dashboard** or **Scores** page to identify trends

### Step 5: Identify and Fix Issues

Common patterns and fixes:

| Issue | Fix |
|---|---|
| Classifier always returns VECTOR | Add few-shot SQL and HYBRID examples to classifier prompt |
| SQL queries return no results | Verify column names in SQL Query Builder match the database schema |
| Vector results irrelevant | Lower score threshold or increase Top K in Knowledge Retrieval |
| Answers hallucinate facility names | Reinforce "only mention facilities in retrieved results" in system prompt |
| HYBRID only uses one method | Verify both branches are connected in the IF/ELSE routing |

### Step 6: A/B Testing

Use the built-in `--label` and `--api-key` flags to compare two chatflow configurations side by side.

#### 6a. Create a variant in Dify

1. Open your chatflow in Dify Studio
2. Click **...** → **Duplicate** to create a copy (e.g., `Nursing Home Demo v2`)
3. Make your change — for example, increase Top K from 3 to 10, change the chunk size, swap the LLM model, or edit the system prompt
4. **Publish** the new chatflow
5. Go to **API Access** and copy the new API key (e.g., `app-NEWKEY...`)

#### 6b. Run both variants

```bash
cd outputs

# Run variant A (baseline — uses the default API key)
python evaluate_demo.py --label v1-baseline

# Run variant B (your change — override the API key)
python evaluate_demo.py --label v2-topk10 --api-key app-NEWKEY...
```

Each run produces timestamped files in `eval_runs/`:
- `eval_YYYYMMDD_HHMMSS_v1-baseline.csv` / `.txt`
- `eval_YYYYMMDD_HHMMSS_v2-topk10.csv` / `.txt`

> **Tip:** Use `--mock` to test the pipeline end-to-end without calling the live API.

#### 6c. Compare results

```bash
python compare_ab.py eval_runs/eval_*_v1-baseline.csv eval_runs/eval_*_v2-topk10.csv
```

You can also provide explicit labels and output path:
```bash
python compare_ab.py eval_runs/eval_A.csv eval_runs/eval_B.csv \
    --label-a v1-baseline --label-b v2-topk10 \
    --output eval_runs/my_comparison.txt
```

#### 6d. Interpret the comparison report

The report includes:

| Section | What to look for |
|---|---|
| **Overall Accuracy** | Net accuracy difference between A and B |
| **Per-Type Accuracy** | Which retrieval types improved or regressed |
| **Per-Query Comparison** | Counts of improved, regressed, and unchanged queries |
| **Improved / Regressed Queries** | Specific queries that changed — inspect these in Langfuse |
| **Recommendation** | `ADOPT B`, `KEEP A`, or `NO CLEAR WINNER` (threshold: 2 percentage points) |

If the recommendation is `ADOPT B`, promote variant B to production. If `NO CLEAR WINNER`, consider evaluating on additional queries or with manual Langfuse scoring before deciding.

---

## Results — Latest Run

**Date:** 2026-02-23
**Mode:** Live (Dify API)

### Overall Accuracy: 90.9%

Exceeds the 85% target.

### Per-Type Accuracy

| Type | Total | Errors | Evaluated | Correct | Accuracy |
|---|---|---|---|---|---|
| SQL | 5 | 1 | 4 | 3 | 75.0% |
| VECTOR | 5 | 2 | 3 | 3 | 100.0% |
| HYBRID | 5 | 1 | 4 | 4 | 100.0% |
lang
### Confusion Matrix

| Expected \ Detected | HYBRID | SQL | VECTOR |
|---|---|---|---|
| HYBRID | 4 | 0 | 0 |
| SQL | 1 | 3 | 0 |
| VECTOR | 0 | 0 | 3 |

### Key Findings

1. **VECTOR and HYBRID classification are excellent** — 100% accuracy on both
2. **SQL has one misclassification** — "Find all facilities in TX with overall_rating less than 2" was classified as HYBRID instead of SQL. This is borderline acceptable since it could reasonably use both methods.
3. **4 API errors (400)** — caused by aggregation queries generating invalid Supabase URLs or token limit issues, not classifier failures
4. **No hallucination issues** in successful responses — all returned real facility names and data

### Recommendations

- Add few-shot examples to the classifier prompt to improve SQL classification edge cases
- Monitor the 400 errors — they stem from Supabase REST API limitations with aggregation queries
- Consider increasing the response LLM's token limit for queries that return large datasets

---

## File Reference

| File | Purpose |
|---|---|
| `outputs/golden_test_set.csv` | 15 curated test queries with expected results |
| `outputs/evaluate_demo.py` | Automated evaluation script (supports `--label` and `--api-key` for A/B testing) |
| `outputs/compare_ab.py` | A/B comparison script — compares two evaluation CSVs side by side |
| `outputs/eval_runs/eval_YYYYMMDD_HHMMSS.csv` | Per-query detailed results from the run |
| `outputs/eval_runs/eval_YYYYMMDD_HHMMSS.txt` | Formatted summary report from the run |
